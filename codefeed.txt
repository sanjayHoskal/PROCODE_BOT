Phase 1: Foundation & Knowledge Base (RAG)
 

Before the agent can "think," it needs access to your business rules and data. We will set up the environment and the vector database.

Step 1.1: Environment Setup: Create the virtual environment and populate the .env file with API keys (Groq, Qdrant, Brevo, LlamaCloud).

Step 1.2: The RAG Ingestion Script: Create a standalone script (outside the API) to ingest your "Past Projects" or "Pricing Policy" PDFs.

Use LlamaParse to clean the data.

Embed it and upload it to Qdrant.

Step 1.3: The Retrieval Tool (rag.py): Write the Python function that the Agent will call to query Qdrant (e.g., retrieve_similar_projects(query)).

 

Phase 2: Deterministic Tools (The "Hands")
 

We need to build the strict logic tools. These are standard Python functions that the LLM will "call" but not "perform."

Step 2.1: Pricing Engine (pricing.py): Implement the math logic.

Input: hours, resource_level.

Output: Final price (Integer).

Step 2.2: PDF Generator (pdf_gen.py): Implement WeasyPrint.

Input: HTML string.

Output: A PDF file saved locally (for now).

Step 2.3: Email Service (emailer.py): Implement Brevo.

Input: PDF filepath, Recipient Email.

Action: Sends the email via SMTP/API.

 

Phase 3: The Agent Brain (LangGraph)
 

This is the most critical phase where we wire the LLM (Groq) to the tools using LangGraph.

Step 3.1: Define State: Define the AgentState class (what data passes between nodes: user messages, calculated price, file paths).

Step 3.2: Define Nodes: Create the functions for the graph:

Node 1 (Reasoning): LLM decides what to do.

Node 2 (Action): Execute tools (Pricing, RAG).

Node 3 (Drafting): LLM writes the HTML proposal.

Step 3.3: Define Edges: Connect the nodes (e.g., If tool called $\rightarrow$ go to Action Node $\rightarrow$ return to Reasoning).

Step 3.4: Test the Brain: Run agent.py via terminal to simulate a conversation without the UI.

 

Phase 4: The API Layer (FastAPI)
 

Now we wrap the working agent in a web server so the frontend can talk to it.

Step 4.1: Server Setup (server.py): Initialize FastAPI.

Step 4.2: Connect Endpoint: Create a POST endpoint /generate_proposal that takes user input and runs the LangGraph workflow.

Step 4.3: Handling Files: Ensure the API can return the generated PDF file or the status text.

 

Phase 5: The Frontend (Streamlit)
 

Finally, we build the user interface.

Step 5.1: UI Layout: Create the text input and file uploader for the screenshot.

Step 5.2: API Integration: Use Python's requests library in Streamlit to send the user input to your FastAPI backend.

Step 5.3: Display Results: Show the "Proposal Summary" and the "Download PDF" button.

 

Phase 6: Deployment (Docker & HF Spaces)
 

Step 6.1: Dockerize: Write the Dockerfile to bundle the Python app and WeasyPrint dependencies (Linux libraries).

Step 6.2: Push to Hub: Deploy the backend to Hugging Face Spaces and the frontend to Streamlit Cloud.



folder structure:

Your-Project-Root/
â”‚
â”œâ”€â”€ setup_structure.py        (You can delete this now)
â”‚
â”œâ”€â”€ backend/                  (Your API Logic)
â”‚   â”‚__ codefeed (saved contents)
â”‚   â”‚â”€â”€ .env                  (We created this in the last step)
â”‚   â”‚â”€â”€ .gitignore
â”‚   â”‚â”€â”€ Dockerfile
â”‚   â”‚â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ ingest.py         <-- For uploading PDFs to Qdrant
â”‚   â”‚__ Knowledge_base
â”‚   â””â”€â”€ app/
â”‚       â”‚â”€â”€ init.py
â”‚       â”‚â”€â”€ config.py         <-- Validation script
â”‚       â”‚â”€â”€ server.py         <-- FastAPI Main
â”‚       â”‚â”€â”€ agent.py          <-- LangGraph Logic
â”‚       â”‚â”€â”€ state.py          <-- Data Schemas
â”‚       â”‚
â”‚       â”œâ”€â”€ prompts/
â”‚       â”‚   â””â”€â”€ system.md
â”‚       â”‚
â”‚       â””â”€â”€ tools/
â”‚           â”‚â”€â”€ init.py
â”‚           â”‚â”€â”€ pricing.py
â”‚           â”‚â”€â”€ pdf_gen.py
â”‚           â”‚â”€â”€ emailer.py
â”‚           â””â”€â”€ rag.py
â”‚___venv
â””â”€â”€ frontend/                 (Your UI Logic)
    â”‚â”€â”€ app.py
    â”‚â”€â”€ requirements.txt
    â””â”€â”€ assets/



*** backend/scripts/ingest.py

import os
import asyncio
from dotenv import load_dotenv
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from qdrant_client import QdrantClient, models
from langchain_core.documents import Document

# Load environment variables
# --- FIX: Dynamic Path Resolution ---
# 1. Get the path of the current script (backend/scripts/ingest.py)
current_script = os.path.abspath(__file__)

# 2. Go up two levels to find the project root (procode_bot/)
backend_dir = os.path.dirname(os.path.dirname(current_script))
project_root = os.path.dirname(backend_dir)

# 3. Define the path to .env
env_path = os.path.join(project_root, ".env")

# 4. Load it and print status
print(f" Loading .env from: {env_path}")
if load_dotenv(env_path):
    print(" .env loaded successfully.")
else:
    print(" Failed to load .env. Check if the file exists at the path above.")

# Verify Keys specifically
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

# Redefine logic for data directory relative to backend
DATA_DIR = os.path.join(backend_dir, "knowledge_base")
COLLECTION_NAME = "procode_knowledge"

async def ingest_data():
    print(f" Loading documents from {DATA_DIR}...")

    # 1. Initialize Parser
    parser = LlamaParse(
        api_key=LLAMA_CLOUD_API_KEY,
        result_type="markdown",
        verbose=True,
    )

    # 2. Find Files
    files = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]
    if not files:
        print(" No PDF files found.")
        return

    documents = []
    
    # 3. Parse Files
    for file in files:
        file_path = os.path.join(DATA_DIR, file)
        print(f" Parsing {file}...")
        try:
            parsed = await parser.aload_data(file_path) # efficient async loading
            text = "\n".join([doc.text for doc in parsed])
            documents.append(Document(page_content=text, metadata={"source": file}))
            print(f" Successfully parsed {file}")
        except Exception as e:
            print(f" Error reading {file}: {e}")

    if not documents:
        print("No documents to process.")
        return

    # 4. Split Text
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    final_docs = splitter.split_documents(documents)
    print(f" Total text chunks created: {len(final_docs)}")

    # 5. Initialize Embeddings & Client
    # BAAI/bge-small-en-v1.5 produces vectors of size 384
    embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5") 
    
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    # 6. Check/Create Collection Explicitly (The Fix)
    # We do this manually to avoid the 'init_from' error in LangChain
    if not client.collection_exists(COLLECTION_NAME):
        print(f" Collection '{COLLECTION_NAME}' does not exist. Creating it...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=384,  # Matches FastEmbed BGE-small
                distance=models.Distance.COSINE
            )
        )
        print(" Collection created.")
    else:
        print(f" Collection '{COLLECTION_NAME}' already exists. Appending data...")

    # 7. Upload to Qdrant
    try:
        # We use the instance wrapper instead of class method 'from_documents'
        vector_store = Qdrant(
            client=client,
            collection_name=COLLECTION_NAME,
            embeddings=embeddings,
        )
        
        vector_store.add_documents(final_docs)
        print(f" SUCCESS: Uploaded {len(final_docs)} vectors to Qdrant!")
    except Exception as e:
        print(f" ERROR uploading to Qdrant: {e}")

# -----------------------
# RUN THE SCRIPT
# -----------------------
if __name__ == "__main__":
    if not QDRANT_API_KEY:
        print(" Missing QDRANT_API_KEY in .env")
    elif not LLAMA_CLOUD_API_KEY:
        print(" Missing LLAMA_CLOUD_API_KEY in .env")
    else:
        asyncio.run(ingest_data())


#agent.py

import os
import sys
import re
from dotenv import load_dotenv

# --- 1. LOAD ENVIRONMENT VARIABLES FIRST ---
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
root_dir = os.path.dirname(backend_dir)
env_path = os.path.join(root_dir, ".env")

print(f"ðŸ”Œ Loading environment from: {env_path}")
load_dotenv(env_path)

# --- 2. IMPORTS ---
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END

from app.state import AgentState
from app.tools.rag import retrieve_similar_projects
from app.tools.pricing import calculate_project_price
from app.tools.pdf_gen import create_pdf
from app.tools.emailer import send_proposal_email

# Initialize Brain
llm = ChatGroq(
    api_key=os.getenv("GROQ_API_KEY"),
    model_name="llama-3.3-70b-versatile",
    temperature=0.3
)

# --- SYSTEM PROMPT (STRICTER) ---
SYSTEM_PROMPT = """You are ProCode Bot, an expert AI consultant.

YOUR PROCESS (FOLLOW STRICTLY):
1. GATHER INFO: Ask about features, user traffic, and platform (Web/Mobile).
2. RESEARCH: If asked about past work/pricing policies, use [LOOKUP: query].
3. ESTIMATE: Once you understand the scope, estimate hours (e.g., Simple=50h, Mid=100h, Complex=300h) and resource level.
4. CALCULATE: Use the tool [CALCULATE: hours, level] to get the exact price.
5. PROPOSE: Present the calculated price to the user.
6. CLOSE: ONLY IF the user accepts the price AND provides an email, use [GENERATE_PROPOSAL].

RULES:
- DO NOT use [CALCULATE] until you have asked about features.
- DO NOT use [GENERATE_PROPOSAL] if you haven't successfully calculated a price yet.
- If a tool fails, tell the user you are having trouble and ask for details again.
"""

# --- NODE 1: REASONING ---
def chatbot_node(state: AgentState):
    messages = state['messages']
    
    if not messages or not isinstance(messages[0], SystemMessage):
        messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages

    instructions = """
    TOOLS AVAILABLE:
    - [LOOKUP: search_term] -> Search past projects.
    - [CALCULATE: hours, level] -> e.g., [CALCULATE: 50, junior] or [CALCULATE: 100, senior].
    - [GENERATE_PROPOSAL] -> Generate PDF and email it.
    """
    
    response = llm.invoke(messages + [SystemMessage(content=instructions)])
    
    next_step = "wait_for_user"
    content = response.content
    
    if "[LOOKUP:" in content:
        next_step = "run_rag"
    elif "[CALCULATE:" in content:
        next_step = "run_pricing"
    elif "[GENERATE_PROPOSAL]" in content:
        next_step = "draft_proposal"
        
    return {
        "messages": [response],
        "next_step": next_step
    }

# --- NODE 2: ACTION (ROBUST PARSING) ---
def tool_node(state: AgentState):
    last_message = state['messages'][-1].content
    
    if "run_rag" in state['next_step']:
        try:
            query = last_message.split("[LOOKUP:")[1].split("]")[0].strip()
            data = retrieve_similar_projects(query)
            return {
                "messages": [AIMessage(content=f"RAG RESULT: {data}")], 
                "rag_context": data,
                "next_step": "chatbot"
            }
        except Exception as e:
             return {"messages": [AIMessage(content=f"System Error in RAG: {e}")], "next_step": "chatbot"}

    elif "run_pricing" in state['next_step']:
        try:
            # ROBUST PARSING: Extract numbers using Regex
            params_text = last_message.split("[CALCULATE:")[1].split("]")[0]
            
            # Find the first number in the string (hours)
            import re
            hours_match = re.search(r'\d+', params_text)
            hours = int(hours_match.group()) if hours_match else 50 # Default to 50 if parsing fails
            
            # Find level
            level = "mid"
            if "senior" in params_text.lower(): level = "senior"
            elif "junior" in params_text.lower(): level = "junior"
            elif "expert" in params_text.lower(): level = "expert"

            price = calculate_project_price(hours, level)
            
            result_msg = f"PRICING TOOL: Calculated Cost: ${price} (for {hours} hours @ {level} level)."
            return {
                "messages": [AIMessage(content=result_msg)],
                "project_price": price,
                "next_step": "chatbot"
            }
        except Exception as e:
            return {"messages": [AIMessage(content=f"Error calculating price. Please ensure you provided hours and level. Details: {e}")], "next_step": "chatbot"}

    return {"next_step": "chatbot"}

# --- NODE 3: DRAFTING ---
def proposal_node(state: AgentState):
    # Fallbacks if data is missing
    price = state.get("project_price", "TBD (Pending Discussion)")
    reqs = "Client Project"
    if len(state['messages']) > 2:
        reqs = state['messages'][-2].content

    # Extract email safely
    recipient = "sanjuhoskal@gmail.com" # Fallback
    for m in reversed(state['messages']):
        if "@" in m.content and "ProCode" not in m.content:
            # Simple regex to find email
            email_match = re.search(r'[\w\.-]+@[\w\.-]+', m.content)
            if email_match:
                recipient = email_match.group(0)
            break

    prompt = f"""
    Write a clean HTML proposal.
    - Requirements Summary: {reqs}
    - Total Price: ${price}
    - Notes: {state.get('rag_context', 'Standard terms apply.')}
    
    Structure: 
    <h1>ProCode Proposal</h1>
    <p>Dear Customer,</p>
    ... details ...
    <div style='background:#eee; padding:10px;'><b>Total Estimate: ${price}</b></div>
    """
    
    html_response = llm.invoke([HumanMessage(content=prompt)])
    html_content = html_response.content
    
    if "```html" in html_content:
        html_content = html_content.split("```html")[1].split("```")[0]

    pdf_path = create_pdf(html_content)
    email_status = send_proposal_email(pdf_path, recipient)
    
    final_msg = f"Proposal generated for ${price} and sent to {recipient}!"
    
    return {
        "messages": [AIMessage(content=final_msg)],
        "pdf_path": pdf_path,
        "next_step": "end"
    }

# --- GRAPH SETUP ---
def route_step(state: AgentState):
    step = state.get("next_step")
    if step in ["run_rag", "run_pricing"]: return "tools"
    elif step == "draft_proposal": return "proposal"
    return END

workflow = StateGraph(AgentState)
workflow.add_node("chatbot", chatbot_node)
workflow.add_node("tools", tool_node)
workflow.add_node("proposal", proposal_node)

workflow.set_entry_point("chatbot")
workflow.add_conditional_edges("chatbot", route_step, {"tools": "tools", "proposal": "proposal", END: END})
workflow.add_edge("tools", "chatbot")
workflow.add_edge("proposal", END)

app = workflow.compile()

if __name__ == "__main__":
    print("ðŸ¤– ProCode Bot is online! (Type 'quit' to exit)")
    while True:
        try:
            user_input = input("You: ")
            if user_input.lower() in ["quit", "exit"]: break
            result = app.invoke({"messages": [HumanMessage(content=user_input)]})
            print(f"Bot: {result['messages'][-1].content}\n")
        except Exception as e:
            print(f"Error: {e}")


#rag.py

import os
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from qdrant_client import QdrantClient

# Load env vars
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "procode_knowledge"

def retrieve_similar_projects(query:str):
    """
    Searches the knowledge base for relevant past projects or policies.
    """
    print(f"RAG Tool Called: Searching for '{query}'...")
    try:
        #1. Connect to qdrant
        client=QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

        #2. Initialise embeddings model
        embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")

        #3. Create Query vector
        query_vector = embeddings.embed_query(query)

        #4. Perform Search (Using new query_points API)
        search_result = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=3,
        ).points

        if not search_result:
            return f"No results found for {query}"
        
        # 5. Format the Output
        results = []
        for hit in search_result:
            #Safely get content from payload
            content = hit.payload.get("page_content","No content available")
            source=hit.payload.get("metadata",{}).get("source","Unknown")
            results.append(f"--- Snippet from {source} ---\n{content}")

        return "\n\n".join(results)
    except Exception as e:
        print(f"RAG Error: {e}")
        return f"Error retrieving similar projects: {str(e)}"
    
if __name__ == "__main__":
    from dotenv import load_dotenv
    #Fix path to load .env correctly for testing
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    load_dotenv(os.path.join(os.path.dirname(BASE_DIR), ".env"))
    print(retrieve_similar_projects('Project pricing'))


#emailer.py

import os
import base64
from dotenv import load_dotenv
import sib_api_v3_sdk
from sib_api_v3_sdk.rest import ApiException

#load env vars
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
ENV_PATH=os.path.join(os.path.dirname(BASE_DIR),'.env')
load_dotenv(ENV_PATH)

#configuration
BREVO_API_KEY = os.getenv('BREVO_API_KEY')
SENDER_EMAIL = os.getenv('SENDER_EMAIL')
SENDER_NAME = os.getenv('SENDER_NAME', "Procode Bot")

def send_proposal_email(pdf_path:str, recipient_email:str):
    """
    sends the proposal PDF to the user via Brevo (Sendinblue).
    Args:
        pdf_path(str): The local path to the generated pdf.
        recipient_email(str): The email address of the user.
        
    Returns:
        dict: The API response or error status.
    """

    if not BREVO_API_KEY:
        raise {"status":"error","message":"Missing API keys or sender email in .env"}
    print(f"Preparing to send email to {recipient_email}....")

    #configuration API client
    configuration = sib_api_v3_sdk.Configuration()
    configuration.api_key['api-key'] = BREVO_API_KEY
    api_instance = sib_api_v3_sdk.TransactionalEmailsApi(sib_api_v3_sdk.ApiClient(configuration))

    #Prepare the attachment
    try:
        with open(pdf_path,'rb') as f:
            pdf_content = f.read()
            encoded_content = base64.b64encode(pdf_content).decode("utf-8")

        filename = os.path.basename(pdf_path)
    except FileNotFoundError:
        return {'status':'error','message':f'File "{pdf_path}" not found.'}
    
    #construct the email object
    send_smtp_email = sib_api_v3_sdk.SendSmtpEmail(
        to=[{"email":recipient_email}],
        sender={"name":SENDER_NAME,"email":SENDER_EMAIL},
        subject="Your Custom Project Proposal - ProCode Bot",
        html_content="""
            <html>
                <body>
                    <h2>Hello!</h2>
                    <p>Please find attached the project proposal we generated based on your requirements.</p>
                    <p>Best regards,<br>ProCode Team</p>
                </body>
            </html>
        """,
        attachment=[{
            'content':encoded_content,
            'name':filename
        }]
    )

    try:
        api_response = api_instance.send_transac_email(send_smtp_email)
        print(f"Email sent successfully! Message ID: {api_response.message_id}")
        return {'status':'success','message_id':api_response.message_id}
    except ApiException as e:
        print(f"Error sending email: {e}")
        return {'status':'error','message':str(e)}
    

# --- Test Block ---
if __name__ == "__main__":
    # 1. Create a dummy file to test
    test_pdf = os.path.join(BASE_DIR, "generated_proposals", "test_email.pdf")
    if not os.path.exists(test_pdf):
        os.makedirs(os.path.dirname(test_pdf), exist_ok=True)
        with open(test_pdf, "w") as f:
            f.write("Dummy PDF Content")

    # 2. Enter YOUR email here to test
    my_email = "sanjuhoskal@gmail.com" 
    
    # FIX 4: Logic corrected. We just call the function directly now.
    send_proposal_email(test_pdf, my_email)


#pdf_gen.py

import os
import uuid
from weasyprint import HTML, CSS

# Path to save the pdf

BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
OUTPUT_FOLDER = os.path.join(BASE_DIR, "generated_proposals")
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# Basic Professional Styling
DEFAULT_CSS = CSS(string="""
    @page { size: A4; margin: 2.5cm; }
    body { font-family: 'Helvetica', 'Arial', sans-serif; font-size: 12pt; line-height: 1.6; color: #333; }
    h1 { color: #2c3e50; border-bottom: 2px solid #2c3e50; padding-bottom: 10px; margin-top: 0; }
    h2 { color: #2980b9; margin-top: 20px; }
    .header { text-align: right; font-size: 10pt; color: #7f8c8d; margin-bottom: 30px; }
    .footer { position: fixed; bottom: 0; right: 0; font-size: 10pt; color: #bdc3c7; }
    .price-box { background: #f8f9fa; border-left: 5px solid #27ae60; padding: 15px; margin: 20px 0; font-weight: bold; }
    table { width: 100%; border-collapse: collapse; margin-top: 20px; }
    th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
    th { background-color: #f2f2f2; }
""")

def create_pdf(html_content:str, filename:str=None) -> str:
    """
    Converts an HTML string into a pdf file.
    Args: 
        html_content(str): The full HTML string of the proposal.
        filename(str): Optional filename. If None, a unique ID is generated.
        
    Returns:
        str: The absolute file path of the generated path
    """

    #1 Generate Filename

    if not filename:
        filename = f"proposal_{uuid.uuid4().hex[:8]}.pdf"

    if not filename.endswith(".pdf"):
        filename += ".pdf"

    file_path = os.path.join(OUTPUT_FOLDER,filename)
    print(f"Generating pdf: {filename}...")
    try:
        HTML(string=html_content).write_pdf(file_path, stylesheets=[DEFAULT_CSS])
        print(f"PDF saved at:{file_path}")
        return file_path
    
    except Exception as e:
        print(f"Error generating PDF: {str(e)}")
        return None
    
#test block
# --- Test Block ---
if __name__ == "__main__":
    # Test HTML content
    test_html = """
    <div class="header">ProCode Bot | AI Generated Proposal</div>
    <h1>Project Proposal: Website Bot</h1>
    <p>Dear Client,</p>
    <p>Based on our analysis, here is the suggested implementation for your website bot.</p>
    
    <h2>Cost Estimate</h2>
    <div class="price-box">
        Total Estimated Cost: $12,500
    </div>
    
    <h2>Timeline</h2>
    <ul>
        <li>Phase 1: Discovery (1 Week)</li>
        <li>Phase 2: Development (4 Weeks)</li>
    </ul>
    """
    
    create_pdf(test_html, "test_proposal.pdf")


#pricing.py

def calculate_project_price(estimated_hours: int, resource_levl: str="mid") -> int:
    """
    calculates the total cost based on hours and developer seniority.
    Args:
        estimated_hours (int): number of hours the project will take.
        resource_level (str): The complexity/seniority required ('junior', 'mid', 'senior', 'expert').
    Returns:
        int: The total calculated price in INR.
    """

# Define hourly rates for different levels
    RATES={
        "junior":100,
        "mid":400,
        "senior":600,
        "expert":1000
    }

    # Normalize input
    level=resource_levl.lower().strip()

    # partial matching: if 'senior' is in string. map it to 'senior'
    if "expert" in level:
        rate=RATES["expert"]
    elif "senior" in level:
        rate=RATES["senior"]
    elif "junior" in level:
        rate=RATES["junior"]
    else:
        rate=RATES["mid"] #default to mid if unclear

    # Price calculator

    total_price=estimated_hours*rate
    return total_price

if __name__=="__main__":
    hours=100
    level='Senior Developer'
    price=calculate_project_price(hours,level)
    print(f"The total price for a {hours} hour project with a {level} is estimated Rs.{price}, !! Please contact us for exact pricing and attractive discounts !! ")


#state.py

import operator
from typing import Annotated, List, TypedDict, Union
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    """The "Clipboard' passed between all nodes in the graph.
    """

    #Conversation history
    #'Operator.add' means: when a node returns a new message, Append it into this list
    # (don't override the old ones)

    messages: Annotated[List[BaseMessage],operator.add]

    #user details extracted from conversation
    user_email : str                
    user_requirements: str

    #Internal data (Hidden from user, used by tools)
    rag_content: str                   #Data found in pdf knowledge base
    project_price: int                 #calculated pricing.py

    #Artifacts
    pdf_path: str                      #path to generated PDF file

    #Control Flags
    next_step: str                     #Tells the graph where to go next



