folder structure:

Your-Project-Root/
â”‚
â”œâ”€â”€ setup_structure.py        (You can delete this now)
â”‚
â”œâ”€â”€ backend/                  (Your API Logic)
â”‚   â”‚__ codefeed (saved contents)
â”‚   â”‚â”€â”€ .env                  (We created this in the last step)
â”‚   â”‚â”€â”€ .gitignore
â”‚   â”‚â”€â”€ Dockerfile
â”‚   â”‚â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ ingest.py         <-- For uploading PDFs to Qdrant
â”‚   â”‚__ Knowledge_base
â”‚   â””â”€â”€ app/
â”‚       â”‚â”€â”€ init.py
â”‚       â”‚â”€â”€ config.py         <-- Validation script
â”‚       â”‚â”€â”€ server.py         <-- FastAPI Main
â”‚       â”‚â”€â”€ agent.py          <-- LangGraph Logic
â”‚       â”‚â”€â”€ state.py          <-- Data Schemas
â”‚       â”‚
â”‚       â”œâ”€â”€ prompts/
â”‚       â”‚   â””â”€â”€ system.md
â”‚       â”‚
â”‚       â””â”€â”€ tools/
â”‚           â”‚â”€â”€ init.py
â”‚           â”‚â”€â”€ pricing.py
â”‚           â”‚â”€â”€ pdf_gen.py
â”‚           â”‚â”€â”€ emailer.py
â”‚           â””â”€â”€ rag.py
â”‚___venv
â””â”€â”€ frontend/                 (Your UI Logic)
    â”‚â”€â”€ app.py
    â”‚â”€â”€ requirements.txt
    â””â”€â”€ assets/



*** backend/scripts/ingest.py

import os
import asyncio
from dotenv import load_dotenv
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from qdrant_client import QdrantClient, models
from langchain_core.documents import Document

# Load environment variables
# --- FIX: Dynamic Path Resolution ---
# 1. Get the path of the current script (backend/scripts/ingest.py)
current_script = os.path.abspath(__file__)

# 2. Go up two levels to find the project root (procode_bot/)
backend_dir = os.path.dirname(os.path.dirname(current_script))
project_root = os.path.dirname(backend_dir)

# 3. Define the path to .env
env_path = os.path.join(project_root, ".env")

# 4. Load it and print status
print(f" Loading .env from: {env_path}")
if load_dotenv(env_path):
    print(" .env loaded successfully.")
else:
    print(" Failed to load .env. Check if the file exists at the path above.")

# Verify Keys specifically
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

# Redefine logic for data directory relative to backend
DATA_DIR = os.path.join(backend_dir, "knowledge_base")
COLLECTION_NAME = "procode_knowledge"

async def ingest_data():
    print(f" Loading documents from {DATA_DIR}...")

    # 1. Initialize Parser
    parser = LlamaParse(
        api_key=LLAMA_CLOUD_API_KEY,
        result_type="markdown",
        verbose=True,
    )

    # 2. Find Files
    files = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]
    if not files:
        print(" No PDF files found.")
        return

    documents = []
    
    # 3. Parse Files
    for file in files:
        file_path = os.path.join(DATA_DIR, file)
        print(f" Parsing {file}...")
        try:
            parsed = await parser.aload_data(file_path) # efficient async loading
            text = "\n".join([doc.text for doc in parsed])
            documents.append(Document(page_content=text, metadata={"source": file}))
            print(f" Successfully parsed {file}")
        except Exception as e:
            print(f" Error reading {file}: {e}")

    if not documents:
        print("No documents to process.")
        return

    # 4. Split Text
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    final_docs = splitter.split_documents(documents)
    print(f" Total text chunks created: {len(final_docs)}")

    # 5. Initialize Embeddings & Client
    # BAAI/bge-small-en-v1.5 produces vectors of size 384
    embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5") 
    
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    # 6. Check/Create Collection Explicitly (The Fix)
    # We do this manually to avoid the 'init_from' error in LangChain
    if not client.collection_exists(COLLECTION_NAME):
        print(f" Collection '{COLLECTION_NAME}' does not exist. Creating it...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=384,  # Matches FastEmbed BGE-small
                distance=models.Distance.COSINE
            )
        )
        print(" Collection created.")
    else:
        print(f" Collection '{COLLECTION_NAME}' already exists. Appending data...")

    # 7. Upload to Qdrant
    try:
        # We use the instance wrapper instead of class method 'from_documents'
        vector_store = Qdrant(
            client=client,
            collection_name=COLLECTION_NAME,
            embeddings=embeddings,
        )
        
        vector_store.add_documents(final_docs)
        print(f" SUCCESS: Uploaded {len(final_docs)} vectors to Qdrant!")
    except Exception as e:
        print(f" ERROR uploading to Qdrant: {e}")

# -----------------------
# RUN THE SCRIPT
# -----------------------
if __name__ == "__main__":
    if not QDRANT_API_KEY:
        print(" Missing QDRANT_API_KEY in .env")
    elif not LLAMA_CLOUD_API_KEY:
        print(" Missing LLAMA_CLOUD_API_KEY in .env")
    else:
        asyncio.run(ingest_data())

































#backend/app/agent.py

import os
import sys
import re
from dotenv import load_dotenv

# --- 1. LOAD ENVIRONMENT VARIABLES FIRST ---
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
root_dir = os.path.dirname(backend_dir)
env_path = os.path.join(root_dir, ".env")

print(f"ðŸ”Œ Loading environment from: {env_path}")
load_dotenv(env_path)

# --- 2. IMPORTS ---
from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END

from app.state import AgentState
from app.tools.rag import retrieve_similar_projects
from app.tools.pricing import calculate_project_price
from app.tools.pdf_gen import create_pdf
from app.tools.emailer import send_proposal_email

# Initialize Brain
llm = ChatGroq(
    api_key=os.getenv("GROQ_API_KEY"),
    model_name="llama-3.3-70b-versatile",
    temperature=0.3
)

# --- SYSTEM PROMPT (STRICTER) ---
SYSTEM_PROMPT = """You are ProCode Bot, an expert AI consultant.

YOUR PROCESS (FOLLOW STRICTLY):
1. GATHER INFO: Ask about features, user traffic, and platform (Web/Mobile).
2. RESEARCH: If asked about past work/pricing policies, use [LOOKUP: query].
3. ESTIMATE: Once you understand the scope, estimate hours (e.g., Simple=50h, Mid=100h, Complex=300h) and resource level.
4. CALCULATE: Use the tool [CALCULATE: hours, level] to get the exact price.
5. PROPOSE: Present the calculated price to the user.
6. CLOSE: ONLY IF the user accepts the price AND provides an email, use [GENERATE_PROPOSAL].

RULES:
- DO NOT use [CALCULATE] until you have asked about features.
- DO NOT use [GENERATE_PROPOSAL] if you haven't successfully calculated a price yet.
- If a tool fails, tell the user you are having trouble and ask for details again.
"""

# --- NODE 1: REASONING ---
def chatbot_node(state: AgentState):
    messages = state['messages']
    
    if not messages or not isinstance(messages[0], SystemMessage):
        messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages

    instructions = """
    TOOLS AVAILABLE:
    - [LOOKUP: search_term] -> Search past projects.
    - [CALCULATE: hours, level] -> e.g., [CALCULATE: 50, junior] or [CALCULATE: 100, senior].
    - [GENERATE_PROPOSAL] -> Generate PDF and email it.
    """
    
    response = llm.invoke(messages + [SystemMessage(content=instructions)])
    
    next_step = "wait_for_user"
    content = response.content
    
    if "[LOOKUP:" in content:
        next_step = "run_rag"
    elif "[CALCULATE:" in content:
        next_step = "run_pricing"
    elif "[GENERATE_PROPOSAL]" in content:
        next_step = "draft_proposal"
        
    return {
        "messages": [response],
        "next_step": next_step
    }

# --- NODE 2: ACTION (ROBUST PARSING) ---
def tool_node(state: AgentState):
    last_message = state['messages'][-1].content
    
    if "run_rag" in state['next_step']:
        try:
            query = last_message.split("[LOOKUP:")[1].split("]")[0].strip()
            data = retrieve_similar_projects(query)
            return {
                "messages": [AIMessage(content=f"RAG RESULT: {data}")], 
                "rag_context": data,
                "next_step": "chatbot"
            }
        except Exception as e:
             return {"messages": [AIMessage(content=f"System Error in RAG: {e}")], "next_step": "chatbot"}

    elif "run_pricing" in state['next_step']:
        try:
            # ROBUST PARSING: Extract numbers using Regex
            params_text = last_message.split("[CALCULATE:")[1].split("]")[0]
            
            # Find the first number in the string (hours)
            import re
            hours_match = re.search(r'\d+', params_text)
            hours = int(hours_match.group()) if hours_match else 50 # Default to 50 if parsing fails
            
            # Find level
            level = "mid"
            if "senior" in params_text.lower(): level = "senior"
            elif "junior" in params_text.lower(): level = "junior"
            elif "expert" in params_text.lower(): level = "expert"

            price = calculate_project_price(hours, level)
            
            result_msg = f"PRICING TOOL: Calculated Cost: â‚¹{price:,} (for {hours} hours @ {level} level)."
            return {
                "messages": [AIMessage(content=result_msg)],
                "project_price": price,
                "next_step": "chatbot"
            }
        except Exception as e:
            return {"messages": [AIMessage(content=f"Error calculating price. Please ensure you provided hours and level. Details: {e}")], "next_step": "chatbot"}

    return {"next_step": "chatbot"}

# --- NODE 3: DRAFTING ---
# ... (Imports remain the same) ...

# --- NODE 3: DRAFTING (Updated) ---
def proposal_node(state: AgentState):
    # Fallbacks
    price = state.get("project_price", 0) # Default to integer 0
    reqs = "Client Project"
    if len(state['messages']) > 2:
        reqs = state['messages'][-2].content

    # Extract email
    recipient = "sanjuhoskal@gmail.com" 
    for m in reversed(state['messages']):
        if "@" in m.content and "ProCode" not in m.content:
            email_match = re.search(r'[\w\.-]+@[\w\.-]+', m.content)
            if email_match:
                recipient = email_match.group(0)
            break

    # --- HTML TEMPLATE WITH LOGO AND FOOTER ---
    # Note: price:, formats number with commas (e.g. 40,000)
    prompt = f"""
    Write a clean HTML proposal.
    - Requirements Summary: {reqs}
    - Total Price: INR {price:,}
    - Notes: {state.get('rag_context', 'Standard terms apply.')}
    
    REQUIRED HTML STRUCTURE (Do strictly):
    
    <div class="header-container">
        <div class="company-name">ProCode Bot</div>
        <img src="file:///D:/ML_Projects/Procode_Prod_Projects/procode_bot/backend/knowledge_base/procode_image.png" class="logo" alt="Company Logo">
    </div>

    <h1>Project Proposal</h1>
    <p>Dear Customer,</p>
    
    [...Insert specific project details, timeline, and scope here based on requirements...]

    <h2>Commercials</h2>
    <div class="price-box">
        Total Estimated Cost: â‚¹{price:,}
    </div>

    <div class="footer">
        <b>ProCode Technologies Pvt Ltd</b><br>
        123 Innovation Drive, Tech City, Karnataka - 577201<br>
        Contact: +91 98765 43210 | Email: contact@procode.com
    </div>
    """
    
    # Run LLM
    html_response = llm.invoke([HumanMessage(content=prompt)])
    html_content = html_response.content
    
    # Strip Markdown if present
    if "```html" in html_content:
        html_content = html_content.split("```html")[1].split("```")[0]

    # Generate PDF
    pdf_path = create_pdf(html_content)
    
    # Send Email
    email_status = send_proposal_email(pdf_path, recipient)
    
    final_msg = f"Proposal generated for â‚¹{price:,} and sent to {recipient}!"
    
    return {
        "messages": [AIMessage(content=final_msg)],
        "pdf_path": pdf_path,
        "next_step": "end"
    }

# --- GRAPH SETUP ---
def route_step(state: AgentState):
    step = state.get("next_step")
    if step in ["run_rag", "run_pricing"]: return "tools"
    elif step == "draft_proposal": return "proposal"
    return END

workflow = StateGraph(AgentState)
workflow.add_node("chatbot", chatbot_node)
workflow.add_node("tools", tool_node)
workflow.add_node("proposal", proposal_node)

workflow.set_entry_point("chatbot")
workflow.add_conditional_edges("chatbot", route_step, {"tools": "tools", "proposal": "proposal", END: END})
workflow.add_edge("tools", "chatbot")
workflow.add_edge("proposal", END)

app = workflow.compile()

if __name__ == "__main__":
    print("ðŸ¤– ProCode Bot is online! (Type 'quit' to exit)")
    while True:
        try:
            user_input = input("You: ")
            if user_input.lower() in ["quit", "exit"]: break
            result = app.invoke({"messages": [HumanMessage(content=user_input)]})
            print(f"Bot: {result['messages'][-1].content}\n")
        except Exception as e:
            print(f"Error: {e}")














#rag.py

import os
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from qdrant_client import QdrantClient

# Load env vars
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "procode_knowledge"

def retrieve_similar_projects(query:str):
    """
    Searches the knowledge base for relevant past projects or policies.
    """
    print(f"RAG Tool Called: Searching for '{query}'...")
    try:
        #1. Connect to qdrant
        client=QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

        #2. Initialise embeddings model
        embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")

        #3. Create Query vector
        query_vector = embeddings.embed_query(query)

        #4. Perform Search (Using new query_points API)
        search_result = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=3,
        ).points

        if not search_result:
            return f"No results found for {query}"
        
        # 5. Format the Output
        results = []
        for hit in search_result:
            #Safely get content from payload
            content = hit.payload.get("page_content","No content available")
            source=hit.payload.get("metadata",{}).get("source","Unknown")
            results.append(f"--- Snippet from {source} ---\n{content}")

        return "\n\n".join(results)
    except Exception as e:
        print(f"RAG Error: {e}")
        return f"Error retrieving similar projects: {str(e)}"
    
if __name__ == "__main__":
    from dotenv import load_dotenv
    #Fix path to load .env correctly for testing
    BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    load_dotenv(os.path.join(os.path.dirname(BASE_DIR), ".env"))
    print(retrieve_similar_projects('Project pricing'))















#emailer.py

import os
import base64
from dotenv import load_dotenv
import sib_api_v3_sdk
from sib_api_v3_sdk.rest import ApiException

# Load env vars
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
ENV_PATH = os.path.join(os.path.dirname(BASE_DIR), ".env")
load_dotenv(ENV_PATH)

# Configuration
BREVO_API_KEY = os.getenv("BREVO_API_KEY")
SENDER_EMAIL = os.getenv("SENDER_EMAIL")
SENDER_NAME = os.getenv("SENDER_NAME", "Procode Bot")


def send_proposal_email(pdf_path: str, recipient_email):
    """
    Sends the proposal PDF to one or multiple users via Brevo (Sendinblue).

    Args:
        pdf_path (str): Path to the generated PDF.
        recipient_email (str or list): Single email string or a list of emails.

    Returns:
        dict: API response or error status.
    """

    # Validate API key
    if not BREVO_API_KEY or not SENDER_EMAIL:
        return {"status": "error", "message": "Missing API keys or sender email in .env"}

    # Normalize email input (single or multiple)
    if isinstance(recipient_email, str):
        recipient_list = [recipient_email]
    elif isinstance(recipient_email, list):
        recipient_list = recipient_email
    else:
        return {"status": "error", "message": "recipient_email must be a string or a list"}

    print(f"Preparing to send email to: {recipient_list}")

    # Configure API client
    configuration = sib_api_v3_sdk.Configuration()
    configuration.api_key["api-key"] = BREVO_API_KEY
    api_instance = sib_api_v3_sdk.TransactionalEmailsApi(
        sib_api_v3_sdk.ApiClient(configuration)
    )

    # Prepare attachment
    try:
        with open(pdf_path, "rb") as f:
            pdf_content = f.read()
            encoded_content = base64.b64encode(pdf_content).decode("utf-8")

        filename = os.path.basename(pdf_path)

    except FileNotFoundError:
        return {"status": "error", "message": f'File "{pdf_path}" not found.'}

    # Construct email object
    send_smtp_email = sib_api_v3_sdk.SendSmtpEmail(
        to=[{"email": email} for email in recipient_list],
        sender={"name": SENDER_NAME, "email": SENDER_EMAIL},
        subject="Your Custom Project Proposal - ProCode Bot",
        html_content="""
            <html>
                <body>
                    <h2>Hello!</h2>
                    <p>Please find attached the project proposal generated based on your requirements.</p>
                    <p>Best regards,<br>ProCode Team</p>
                </body>
            </html>
        """,
        attachment=[
            {
                "content": encoded_content,
                "name": filename
            }
        ],
    )

    # Send email via Brevo
    try:
        api_response = api_instance.send_transac_email(send_smtp_email)
        print(f"Email sent successfully! Message ID: {api_response.message_id}")

        return {"status": "success", "message_id": api_response.message_id}

    except ApiException as e:
        print(f"Error sending email: {e}")
        return {"status": "error", "message": str(e)}


# --- Test Block ---
if __name__ == "__main__":
    # Create a dummy file for testing
    test_pdf = os.path.join(BASE_DIR, "generated_proposals", "test_email.pdf")
    if not os.path.exists(test_pdf):
        os.makedirs(os.path.dirname(test_pdf), exist_ok=True)
        with open(test_pdf, "w") as f:
            f.write("Dummy PDF Content")

    # Test with multiple emails
    test_emails = ["sanjuhoskal@gmail.com", "sachinsachu9590@gmail.com"]

    send_proposal_email(test_pdf, test_emails)




















#pdf_gen.py

import os
import uuid
from weasyprint import HTML, CSS

# Path to save the pdf
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
OUTPUT_FOLDER = os.path.join(BASE_DIR, "generated_proposals")
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# --- UPDATED CSS ---
DEFAULT_CSS = CSS(string="""
    @page { 
        size: A4; 
        margin: 2cm; 
        @bottom-center { content: element(footer); } /* Standard print footer */
    }
    
    body { 
        font-family: 'Helvetica', 'Arial', sans-serif; 
        font-size: 11pt; 
        line-height: 1.5; 
        color: #333;
    }

    /* Logo Positioning */
    .header-container {
        display: block;
        height: 80px;
        margin-bottom: 20px;
        border-bottom: 2px solid #2c3e50;
    }
    .logo {
        float: right;
        height: 60px; /* Adjust size */
        width: auto;
    }
    .company-name {
        float: left;
        font-size: 20pt;
        font-weight: bold;
        color: #2c3e50;
        margin-top: 15px;
    }

    /* Content Styling */
    h1 { color: #2c3e50; margin-top: 20px; }
    h2 { color: #2980b9; margin-top: 20px; border-bottom: 1px solid #eee; padding-bottom: 5px; }
    
    .price-box { 
        background: #f8f9fa; 
        border-left: 5px solid #27ae60; 
        padding: 15px; 
        margin: 20px 0; 
        font-weight: bold;
        font-size: 1.2em;
    }

    /* Footer Styling */
    .footer {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        height: 50px;
        text-align: center;
        font-size: 9pt;
        color: #7f8c8d;
        border-top: 1px solid #ddd;
        padding-top: 10px;
    }
""")

def create_pdf(html_content:str, filename:str=None) -> str:
    if not filename:
        filename = f"proposal_{uuid.uuid4().hex[:8]}.pdf"
    if not filename.endswith(".pdf"):
        filename += ".pdf"

    file_path = os.path.join(OUTPUT_FOLDER,filename)
    print(f"Generating pdf: {filename}...")
    try:
        # base_url is needed if you use local images
        HTML(string=html_content, base_url=BASE_DIR).write_pdf(file_path, stylesheets=[DEFAULT_CSS])
        print(f"PDF saved at: {file_path}")
        return file_path
    except Exception as e:
        print(f"Error generating PDF: {str(e)}")
        return None















#pricing.py

def calculate_project_price(estimated_hours: int, resource_levl: str="mid") -> int:
    """
    calculates the total cost based on hours and developer seniority.
    Args:
        estimated_hours (int): number of hours the project will take.
        resource_level (str): The complexity/seniority required ('junior', 'mid', 'senior', 'expert').
    Returns:
        int: The total calculated price in INR.
    """

# Define hourly rates for different levels
    RATES={
        "junior":100,
        "mid":250,
        "senior":500,
        "expert":1000
    }

    # Normalize input
    level=resource_levl.lower().strip()

    # partial matching: if 'senior' is in string. map it to 'senior'
    if "expert" in level:
        rate=RATES["expert"]
    elif "senior" in level:
        rate=RATES["senior"]
    elif "junior" in level:
        rate=RATES["junior"]
    else:
        rate=RATES["mid"] #default to mid if unclear

    # Price calculator

    total_price=estimated_hours*rate
    return total_price

if __name__=="__main__":
    hours=100
    level='Senior Developer'
    price=calculate_project_price(hours,level)
    print(f"The total price for a {hours} hour project with a {level} is estimated Rs.{price}, !! Please contact us for exact pricing and attractive discounts !! ")















#state.py

import operator
from typing import Annotated, List, TypedDict, Union
from langchain_core.messages import BaseMessage

class AgentState(TypedDict):
    """The "Clipboard' passed between all nodes in the graph.
    """

    #Conversation history
    #'Operator.add' means: when a node returns a new message, Append it into this list
    # (don't override the old ones)

    messages: Annotated[List[BaseMessage],operator.add]

    #user details extracted from conversation
    user_email : str                
    user_requirements: str

    #Internal data (Hidden from user, used by tools)
    rag_content: str                   #Data found in pdf knowledge base
    project_price: int                 #calculated pricing.py

    #Artifacts
    pdf_path: str                      #path to generated PDF file

    #Control Flags
    next_step: str                     #Tells the graph where to go next






















# backend/app/server.py

import os
import base64
import io
import uvicorn
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_core.messages import HumanMessage
from langgraph.checkpoint.memory import MemorySaver
from langchain_groq import ChatGroq
from pypdf import PdfReader

# Import the workflow from your agent
# we use relative import since this file is inside the 'app' package
from app.agent import workflow

# Initialize FastAPI
app=FastAPI(title="ProCode Bot API", version="1.1")

# add CORS (Allows your streamlit frontend to talk to this backend)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Add Memory (So the bot remembers context like "Price is $40k")
memory = MemorySaver()
# we compile the graph HERE with checkpointer
agent_app = workflow.compile(checkpointer=memory)

# Vision and PDF helper
def process_file(file_data: str, file_type: str) -> str:
    """
    Takes base64 file data and returns a text description / content.
    """
    try:
        decoded_file = base64.b64decode(file_data)

        # Handle PDF's (Extract Text)
        if "pdf" in file_type.lower():
            pdf_file = io.BytesIO(decoded_file)
            reader = PdfReader(pdf_file)
            text = ""
            for page in reader.pages:
                text+=page.extract_text() + "\n"
            return f"\n[ATTACHED PDF CONTENT]:\n{text[:4000]}..."  #Limit to 4k chars to save tokens
        
        # Handle images (Using Groq vision)
        elif any(x in file_type.lower() for x in ["png","jpg","jpeg"]):
            print(" Analysing Image...")
            vision_llm = ChatGroq(
                api_key = os.getenv("GROQ_API_KEY"),
                model_name = "groq/compound",     #vision model
                temperature=0.1
            )

            # Create a vision message
            msg = HumanMessage(content=[
                {"type":"text", "text": "Describe this UI/Screenshot in technical details for a developer."},
                {"type":"image_url","image_url":{"url":f"data:image/jpeg;base64,{file_data}"}}
                ])
            response = vision_llm.invoke([msg])
            return f"\n[IMAGE ANALYSIS]: The user uploaded a screenshot. Description:\n{response.content}"
        
        return ""  # If not a supported file type
    except Exception as e:
        return f"\n[SYSTEM ERROR]: Could not process file. Error : {e}"
    


# Define request model
class ChatRequest(BaseModel):
    message: str
    thread_id: str = "default_user"   #unique id for each conversation
    file_data: str = None              #base64 encoded string of file data
    file_type: str = None             #mime type of the file

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    try:
            # Process file if provided
        file_context = ""
        if request.file_data and request.file_type:
            file_context = process_file(request.file_data, request.file_type)
 
        # Combine user messages + file context
            full_input = request.message + file_context

        # Define config with thread_id to maintain state
        config = {"configurable": {"thread_id": request.thread_id}}

        # Prepare the input
        #input_message = HumanMessage(content=request.message)

        # Run the agent and get response
        result = agent_app.invoke(
            {"messages": [HumanMessage(content=full_input)]}, config=config
        )

        # extract the bot's last response
        last_message = result["messages"][-1].content

        # Check for PDF in the state
        pdf_path = result.get("pdf_path", None)

        # Return structured response
        return {
            "response": last_message,
            "pdf_path": pdf_path               # will be None unless a PDF was generated
        }
    
    except Exception as e:
        print(f"Server Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    

# 6. Run server (Optional: for debugging purposes only)
if __name__ == "__main__":
    print(" Starting server...")
    uvicorn.run(app, host="0.0.0.0", port=8000)










#frontend/app.py

import streamlit as st
import requests
import os
import uuid
import base64

# CONFIGURATION
API_URL = "http://127.0.0.1:8000/chat"
st.set_page_config(page_title="ProCode Bot", page_icon="ðŸ¤–", layout="wide")

# SESSION STATE INITIALIZATION
if "messages" not in st.session_state:
    st.session_state.messages = []

if "thread_id" not in st.session_state:
    st.session_state.thread_id = str(uuid.uuid4())

with st.sidebar:
    st.header(" Projects Controls")
    st.markdown(" Attachments")

        #5.1 UI Layout: File Uploader
        # Note: Currently, the backend focuses on text, but this UI element is ready
        # for when we add image analytics logic later

    uploaded_file = st.file_uploader("Upload Project Screenshot/Docs", type=["png", "jpg", "pdf"])
    if uploaded_file:
        st.success(f"File '{uploaded_file.name}' attached (Visual only for now).")

    st.divider()

    if st.button("Clear Chat"):
        st.session_state.messages = []
        st.rerun()


# --- MAIN CHAT INTERFACE ---

st.title("ProCode Project Consultant")
st.markdown("Describe your projec idea, and I'll help you estimate and propose a solution")

#Display chat history
# [NEW CODE]
# 1. Display Chat History
# We use 'enumerate' to get a unique index 'i' for every message
for i, message in enumerate(st.session_state.messages):
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # If a past message had a PDF, show the button again
        if message.get("pdf_path"):
             if os.path.exists(message["pdf_path"]):
                 with open(message["pdf_path"], "rb") as f:
                    st.download_button(
                        label="Download Proposal PDF",
                        data=f,
                        file_name="ProCode_Proposal.pdf",
                        mime="application/pdf",
                        key=f"history_btn_{i}" # <--- FIX: Unique Key based on Index
                    )
             else:
                 st.warning("PDF file no longer exists locally.")

#Handle user input
if prompt := st.chat_input("Type your requirements here..."):
    # Add user message to history
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    #Api integration
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                # NEW: Handle file upload
                file_upload =  None
                file_type = None

                # Check if a file sits in sidebar uploader
                if uploaded_file is not None:
                    uploaded_file.seek(0)
                    bytes_data = uploaded_file.getvalue()
                    file_payload = base64.b64encode(bytes_data).decode('utf-8')
                    file_type = uploaded_file.type

                payload = {
                    "message": prompt,
                    "thread_id": st.session_state.thread_id,
                    "file_data": file_payload,
                    "file_type": file_type
                }
                #send POST request to API
                response = requests.post(API_URL, json=payload)

                if response.status_code == 200:
                    data = response.json()
                    bot_text = data.get("response", "No response received.")
                    pdf_path = data.get("pdf_path")  #Extract PDF path if exists
                    st.markdown(bot_text)

                    #Display results and download button
                    if pdf_path and os.path.exists(pdf_path):
                        st.success("Proposal generated successfully!")
                        with open(pdf_path, 'rb') as f:
                            st.download_button(
                                label="Download Proposal PDF",
                                data=f,
                                file_name="Procode_Proposal.pdf",
                                mime='application/pdf',
                        )
                        #add to history with the pdf path
                        st.session_state.messages.append({"role": "assistant", "content": bot_text, "pdf_path": pdf_path})
                    else:
                        st.session_state.messages.append({"role": "assistant", "content": bot_text})
                else:
                    st.error(f"API Error: {response.status_code}")
            except requests.exceptions.ConnectionError:
                st.error("Failed to connect to the server.")
            except Exception as e:
                st.error(f"An error occurred: {e}")
                
        
