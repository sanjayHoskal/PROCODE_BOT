Phase 1: Foundation & Knowledge Base (RAG)
 

Before the agent can "think," it needs access to your business rules and data. We will set up the environment and the vector database.

Step 1.1: Environment Setup: Create the virtual environment and populate the .env file with API keys (Groq, Qdrant, Brevo, LlamaCloud).

Step 1.2: The RAG Ingestion Script: Create a standalone script (outside the API) to ingest your "Past Projects" or "Pricing Policy" PDFs.

Use LlamaParse to clean the data.

Embed it and upload it to Qdrant.

Step 1.3: The Retrieval Tool (rag.py): Write the Python function that the Agent will call to query Qdrant (e.g., retrieve_similar_projects(query)).

 

Phase 2: Deterministic Tools (The "Hands")
 

We need to build the strict logic tools. These are standard Python functions that the LLM will "call" but not "perform."

Step 2.1: Pricing Engine (pricing.py): Implement the math logic.

Input: hours, resource_level.

Output: Final price (Integer).

Step 2.2: PDF Generator (pdf_gen.py): Implement WeasyPrint.

Input: HTML string.

Output: A PDF file saved locally (for now).

Step 2.3: Email Service (emailer.py): Implement Brevo.

Input: PDF filepath, Recipient Email.

Action: Sends the email via SMTP/API.

 

Phase 3: The Agent Brain (LangGraph)
 

This is the most critical phase where we wire the LLM (Groq) to the tools using LangGraph.

Step 3.1: Define State: Define the AgentState class (what data passes between nodes: user messages, calculated price, file paths).

Step 3.2: Define Nodes: Create the functions for the graph:

Node 1 (Reasoning): LLM decides what to do.

Node 2 (Action): Execute tools (Pricing, RAG).

Node 3 (Drafting): LLM writes the HTML proposal.

Step 3.3: Define Edges: Connect the nodes (e.g., If tool called $\rightarrow$ go to Action Node $\rightarrow$ return to Reasoning).

Step 3.4: Test the Brain: Run agent.py via terminal to simulate a conversation without the UI.

 

Phase 4: The API Layer (FastAPI)
 

Now we wrap the working agent in a web server so the frontend can talk to it.

Step 4.1: Server Setup (server.py): Initialize FastAPI.

Step 4.2: Connect Endpoint: Create a POST endpoint /generate_proposal that takes user input and runs the LangGraph workflow.

Step 4.3: Handling Files: Ensure the API can return the generated PDF file or the status text.

 

Phase 5: The Frontend (Streamlit)
 

Finally, we build the user interface.

Step 5.1: UI Layout: Create the text input and file uploader for the screenshot.

Step 5.2: API Integration: Use Python's requests library in Streamlit to send the user input to your FastAPI backend.

Step 5.3: Display Results: Show the "Proposal Summary" and the "Download PDF" button.

 

Phase 6: Deployment (Docker & HF Spaces)
 

Step 6.1: Dockerize: Write the Dockerfile to bundle the Python app and WeasyPrint dependencies (Linux libraries).

Step 6.2: Push to Hub: Deploy the backend to Hugging Face Spaces and the frontend to Streamlit Cloud.



folder structure:

Your-Project-Root/
│
├── setup_structure.py        (You can delete this now)
│
├── backend/                  (Your API Logic)
│   │__ codefeed (saved contents)
│   │── .env                  (We created this in the last step)
│   │── .gitignore
│   │── Dockerfile
│   │── requirements.txt
│   │
│   ├── scripts/
│   │   └── ingest.py         <-- For uploading PDFs to Qdrant
│   │__ Knowledge_base
│   └── app/
│       │── init.py
│       │── config.py         <-- Validation script
│       │── server.py         <-- FastAPI Main
│       │── agent.py          <-- LangGraph Logic
│       │── state.py          <-- Data Schemas
│       │
│       ├── prompts/
│       │   └── system.md
│       │
│       └── tools/
│           │── init.py
│           │── pricing.py
│           │── pdf_gen.py
│           │── emailer.py
│           └── rag.py
│___venv
└── frontend/                 (Your UI Logic)
    │── app.py
    │── requirements.txt
    └── assets/



*** backend/scripts/ingest.py

import os
import asyncio
from dotenv import load_dotenv
from llama_parse import LlamaParse
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from qdrant_client import QdrantClient, models
from langchain_core.documents import Document

# Load environment variables
# --- FIX: Dynamic Path Resolution ---
# 1. Get the path of the current script (backend/scripts/ingest.py)
current_script = os.path.abspath(__file__)

# 2. Go up two levels to find the project root (procode_bot/)
backend_dir = os.path.dirname(os.path.dirname(current_script))
project_root = os.path.dirname(backend_dir)

# 3. Define the path to .env
env_path = os.path.join(project_root, ".env")

# 4. Load it and print status
print(f" Loading .env from: {env_path}")
if load_dotenv(env_path):
    print(" .env loaded successfully.")
else:
    print(" Failed to load .env. Check if the file exists at the path above.")

# Verify Keys specifically
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
LLAMA_CLOUD_API_KEY = os.getenv("LLAMA_CLOUD_API_KEY")

# Redefine logic for data directory relative to backend
DATA_DIR = os.path.join(backend_dir, "knowledge_base")
COLLECTION_NAME = "procode_knowledge"

async def ingest_data():
    print(f" Loading documents from {DATA_DIR}...")

    # 1. Initialize Parser
    parser = LlamaParse(
        api_key=LLAMA_CLOUD_API_KEY,
        result_type="markdown",
        verbose=True,
    )

    # 2. Find Files
    files = [f for f in os.listdir(DATA_DIR) if f.endswith(".pdf")]
    if not files:
        print(" No PDF files found.")
        return

    documents = []
    
    # 3. Parse Files
    for file in files:
        file_path = os.path.join(DATA_DIR, file)
        print(f" Parsing {file}...")
        try:
            parsed = await parser.aload_data(file_path) # efficient async loading
            text = "\n".join([doc.text for doc in parsed])
            documents.append(Document(page_content=text, metadata={"source": file}))
            print(f" Successfully parsed {file}")
        except Exception as e:
            print(f" Error reading {file}: {e}")

    if not documents:
        print("No documents to process.")
        return

    # 4. Split Text
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    final_docs = splitter.split_documents(documents)
    print(f" Total text chunks created: {len(final_docs)}")

    # 5. Initialize Embeddings & Client
    # BAAI/bge-small-en-v1.5 produces vectors of size 384
    embeddings = FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5") 
    
    client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)

    # 6. Check/Create Collection Explicitly (The Fix)
    # We do this manually to avoid the 'init_from' error in LangChain
    if not client.collection_exists(COLLECTION_NAME):
        print(f" Collection '{COLLECTION_NAME}' does not exist. Creating it...")
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=models.VectorParams(
                size=384,  # Matches FastEmbed BGE-small
                distance=models.Distance.COSINE
            )
        )
        print(" Collection created.")
    else:
        print(f" Collection '{COLLECTION_NAME}' already exists. Appending data...")

    # 7. Upload to Qdrant
    try:
        # We use the instance wrapper instead of class method 'from_documents'
        vector_store = Qdrant(
            client=client,
            collection_name=COLLECTION_NAME,
            embeddings=embeddings,
        )
        
        vector_store.add_documents(final_docs)
        print(f" SUCCESS: Uploaded {len(final_docs)} vectors to Qdrant!")
    except Exception as e:
        print(f" ERROR uploading to Qdrant: {e}")

# -----------------------
# RUN THE SCRIPT
# -----------------------
if __name__ == "__main__":
    if not QDRANT_API_KEY:
        print(" Missing QDRANT_API_KEY in .env")
    elif not LLAMA_CLOUD_API_KEY:
        print(" Missing LLAMA_CLOUD_API_KEY in .env")
    else:
        asyncio.run(ingest_data())




*** backend/app/tools/rag.py

import os
from langchain_community.vectorstores import Qdrant
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from qdrant_client import QdrantClient

# Load env vars (We assume they are loaded in the main app, but good to be safe)
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "procode_knowledge"

def retrieve_similar_projects(query: str):
    """
    Searches the knowledge base for relevant past projects or policies.
    Useful when the user asks about pricing, similar work, or company rules.
    """
    print(f"RAG Tool called: Searching for '{query}'...")
    try:
        client=QdrantClient(url=QDRANT_URL,api_key=QDRANT_API_KEY)
        embeddings=FastEmbedEmbeddings(model_name="BAAI/bge-small-en-v1.5")
        vector_store=Qdrant(client=client,collection_name=COLLECTION_NAME,
                            embeddings=embeddings)
        docs=vector_store.similarity_search(query,k=3)

        if not docs:
            return "No relavent information found in the knowledge base"
        
        result_text = "\n\n".join([f"--- Snippet from {d.metadata.get('source', 'Unknown')} ---\n{d.page_content}" for d in docs])
        return result_text
    
    except Exception as e:
        raise ValueError(f"Error searching for similar projects: {e}")
    


# Simple test block to run this file directly
if __name__ == "__main__":
    # You can test this by running: python -m backend.app.tools.rag
    # (Make sure .env is loaded before running directly)
    from dotenv import load_dotenv
    load_dotenv()
    print(retrieve_similar_projects("relieving letter"))
    


